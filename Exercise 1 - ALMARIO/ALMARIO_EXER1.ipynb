{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Libraries\n"
      ],
      "metadata": {
        "id": "fccMRikqBE1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ua0AYqki_oaG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import Lasso\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load and Preprocess the Data"
      ],
      "metadata": {
        "id": "7MgRrgbsBKMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from a CSV file\n",
        "df = pd.read_csv('Iris.csv')  # Replace 'iris.csv' with your actual CSV file path\n",
        "\n",
        "# Preview the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Convert 'Species' column to a categorical variable\n",
        "df['Species'] = pd.factorize(df['Species'])[0]  # Convert categories to numerical values\n",
        "\n",
        "# Define the features (X) and target (y)\n",
        "X = df.drop(columns=['Species'])  # Features: all columns except 'Species'\n",
        "y = df['Species']  # Target: 'Species'\n",
        "\n",
        "# Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(f\"Training set size: {X_train.shape}, Testing set size: {X_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw6Sx94yAgrq",
        "outputId": "da296a3d-83d3-4bcd-85e8-ded4de7b834a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
            "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
            "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
            "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
            "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
            "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n",
            "Training set size: (105, 5), Testing set size: (45, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Selection"
      ],
      "metadata": {
        "id": "D3XfIwfRBNDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SelectKBest (ANOVA F-value) to select the top 2 features\n",
        "select_k_best = SelectKBest(score_func=f_classif, k=2)\n",
        "X_train_selected = select_k_best.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = select_k_best.transform(X_test_scaled)\n",
        "\n",
        "# Get selected feature indices\n",
        "selected_features = select_k_best.get_support(indices=True)\n",
        "print(f\"Selected feature indices: {selected_features}\")\n",
        "\n",
        "# Get F-value scores for all features\n",
        "f_scores = select_k_best.scores_\n",
        "print(f\"Feature scores: {f_scores}\")\n",
        "\n",
        "# Print selected feature names (optional, if column names exist)\n",
        "feature_names = X.columns[selected_features]\n",
        "print(f\"Selected features: {feature_names.tolist()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id75C6AVAmay",
        "outputId": "747fda97-c00a-45dc-f21c-9d4357cd6c20"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected feature indices: [3 4]\n",
            "Feature scores: [343.23576754  74.7572012   31.68871543 712.3739871  526.52691616]\n",
            "Selected features: ['PetalLengthCm', 'PetalWidthCm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapper Method (Recursive Feature Elimination - RFE)"
      ],
      "metadata": {
        "id": "8iuIlx8RBQrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply RFE with Logistic Regression to select the top 2 features\n",
        "rfe = RFE(estimator=LogisticRegression(max_iter=10000), n_features_to_select=2)\n",
        "X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)  # Fit and transform training data\n",
        "X_test_rfe = rfe.transform(X_test_scaled)  # Transform test data\n",
        "\n",
        "# Get selected feature indices\n",
        "selected_features_rfe = rfe.get_support(indices=True)\n",
        "print(f\"Selected feature indices (RFE): {selected_features_rfe}\")\n",
        "\n",
        "# Print selected feature names (optional, if column names exist)\n",
        "feature_names_rfe = X.columns[selected_features_rfe]\n",
        "print(f\"Selected features (RFE): {feature_names_rfe.tolist()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qkASaNEBRKx",
        "outputId": "703bb40a-615c-43ae-b001-eb9bffdd0aeb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected feature indices (RFE): [0 3]\n",
            "Selected features (RFE): ['Id', 'PetalLengthCm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedded Method (Lasso Regression)"
      ],
      "metadata": {
        "id": "2uJ0NnXmBbyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Lasso Regression for feature selection\n",
        "lasso = Lasso(alpha=0.01)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the coefficients\n",
        "lasso_coefficients = lasso.coef_\n",
        "print(f\"Lasso Coefficients: {lasso_coefficients}\")\n",
        "\n",
        "# Select features with non-zero coefficients\n",
        "selected_features_lasso = np.where(lasso_coefficients != 0)[0]\n",
        "print(f\"Selected feature indices (Lasso): {selected_features_lasso}\")\n",
        "\n",
        "# Print selected feature names (optional, if column names exist)\n",
        "feature_names_lasso = X.columns[selected_features_lasso]\n",
        "print(f\"Selected features (Lasso): {feature_names_lasso.tolist()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZpKd5tNBdiy",
        "outputId": "acb55667-7b8d-4505-8909-098ef891076e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso Coefficients: [ 0.30344144  0.         -0.02792404  0.22302779  0.2628675 ]\n",
            "Selected feature indices (Lasso): [0 2 3 4]\n",
            "Selected features (Lasso): ['Id', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Training and Evaluation"
      ],
      "metadata": {
        "id": "BJRHrbAfBlNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# --- For SelectKBest ---\n",
        "# Train the model using the selected features from SelectKBest\n",
        "model.fit(X_train_selected, y_train)\n",
        "y_pred_kbest = model.predict(X_test_selected)\n",
        "accuracy_kbest = accuracy_score(y_test, y_pred_kbest)\n",
        "print(f\"Accuracy using SelectKBest: {accuracy_kbest:.4f}\")\n",
        "\n",
        "# --- For RFE ---\n",
        "# Train the model using the selected features from RFE\n",
        "model.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = model.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "print(f\"Accuracy using RFE: {accuracy_rfe:.4f}\")\n",
        "\n",
        "# --- For Lasso ---\n",
        "# Select features based on Lasso coefficients (non-zero coefficients)\n",
        "lasso_selected_features = np.where(lasso.coef_ != 0)[0]\n",
        "\n",
        "# Transform the training and test sets based on the Lasso selected features\n",
        "X_train_lasso = X_train_scaled[:, lasso_selected_features]\n",
        "X_test_lasso = X_test_scaled[:, lasso_selected_features]\n",
        "\n",
        "# Train the model using the selected features from Lasso\n",
        "model.fit(X_train_lasso, y_train)\n",
        "y_pred_lasso = model.predict(X_test_lasso)\n",
        "accuracy_lasso = accuracy_score(y_test, y_pred_lasso)\n",
        "print(f\"Accuracy using Lasso: {accuracy_lasso:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9C4-sAQBoM6",
        "outputId": "1faf332a-0b43-4ce5-ccd4-eb1beb2fded3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using SelectKBest: 1.0000\n",
            "Accuracy using RFE: 1.0000\n",
            "Accuracy using Lasso: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Comparison"
      ],
      "metadata": {
        "id": "vPQuzcFMCKwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the results in a DataFrame\n",
        "results = pd.DataFrame({\n",
        "    'Method': ['Filter (ANOVA F-value)', 'Wrapper (RFE)', 'Embedded (Lasso)'],\n",
        "    'Accuracy': [accuracy_kbest, accuracy_rfe, accuracy_lasso]\n",
        "})\n",
        "\n",
        "# Display the results\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6OvBM-XCNYx",
        "outputId": "4de6f259-fd93-4d01-b5ed-7082c0146985"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Method  Accuracy\n",
            "0  Filter (ANOVA F-value)       1.0\n",
            "1           Wrapper (RFE)       1.0\n",
            "2        Embedded (Lasso)       1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Code:\n",
        "Import Libraries:\n",
        "\n",
        "We begin by importing necessary libraries such as pandas, sklearn, and numpy. These libraries help us manipulate data, create machine learning models, and perform feature selection.\n",
        "Load and Preprocess the Data:\n",
        "\n",
        "We load the Iris dataset from a CSV file and convert the 'Species' column into numerical values. This is important because machine learning models require numerical data.\n",
        "We then split the data into training (70%) and testing (30%) sets.\n",
        "StandardScaler is applied to standardize the features. This step ensures that the data is scaled so that all features have zero mean and unit variance, which is important for certain machine learning models.\n",
        "Feature Selection:\n",
        "\n",
        "We use three different feature selection methods to select the most important features for classification:\n",
        "\n",
        "Filter Method (ANOVA F-value): This method selects the top 2 features based on their statistical significance (F-value).\n",
        "Wrapper Method (RFE): This method recursively eliminates the least important features by fitting a model at each step (using logistic regression).\n",
        "Embedded Method (Lasso Regression): Lasso applies L1 regularization to the logistic regression model, forcing some feature coefficients to become zero, thus eliminating less important features.\n",
        "After selecting features using each method, we print out the selected features.\n",
        "\n",
        "Model Training and Evaluation:\n",
        "\n",
        "We use Logistic Regression to train a model with the features selected by each method.\n",
        "For each method, we make predictions on the test set and calculate the accuracy of the model.\n",
        "The accuracy score tells us how well the model performs with the selected features.\n",
        "\n",
        "Comparison of Methods:\n",
        "\n",
        "We compare the accuracy scores from each method (SelectKBest, RFE, and Lasso) by storing the results in a DataFrame and printing it. This allows us to see which feature selection method worked best in terms of model accuracy.\n",
        "Key Points to Explain:\n",
        "Feature Selection helps to reduce the number of features while keeping the most important ones. This leads to faster model training and sometimes better performance.\n",
        "Different Methods: We applied three different methods: Filter, Wrapper, and Embedded, each with its strengths.\n",
        "Accuracy Comparison: After training the model with each method, we compare how well they performed based on the accuracy score.\n",
        "This approach helps in choosing the best features for a model and can lead to more efficient and accurate predictions."
      ],
      "metadata": {
        "id": "mB-XCRyfDFUi"
      }
    }
  ]
}